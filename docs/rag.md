# 💬 RAG Querying

LaunchLens includes a lightweight Retrieval-Augmented Generation (RAG) module, enabling users to ask natural language questions about the launch dataset.

This system combines ChromaDB, OpenAI, and LangChain to deliver grounded, context-aware answers.

---

## 📚 What Gets Indexed?

After analysis, the following files are embedded:

- `launch_recommendation.md`
- `top_launchpad_configs.csv`
- `orbit_mass_profiles.csv`
- `success_by_year.csv`

These represent statistical summaries and insights derived from the launch data.

---

## 🔧 How It Works

1. **Document Loading**  
   Text and CSV files are parsed and chunked into semantic blocks using LangChain’s text splitter.

2. **Embedding**  
   OpenAI’s `text-embedding-ada-002` is used to convert each chunk into a vector.

3. **Storage**  
   All embeddings are stored in a local persistent ChromaDB directory (`chroma_store/`).

4. **Retrieval + Answering**  
   At query time, the top `k` relevant chunks are retrieved and passed to `ChatOpenAI`, which synthesizes a final answer.

---

## 🔐 Environment Setup

To enable RAG, add your OpenAI key to a `.env` file in the project root:

```env
OPENAI_API_KEY=sk-...
```

## 🧠 Streamlit Integration

In the "Strategic Launch Planner" tab, users can:

- Ask questions like:
  - “What’s the best orbit and payload for success?”
  - “Which launchpad has the highest success rate?”
- View answers in real time, generated by an LLM grounded in LaunchLens analysis outputs

If no `.env` file is present with a valid OpenAI API key, the feature is gracefully hidden from the interface.


## 🧩 Modular by Design

The RAG functionality in LaunchLens is fully modular and lives in the `rag/` directory:

- `indexer.py`: Embeds and stores analysis outputs into ChromaDB after the pipeline runs
- `query_engine.py`: Provides a simple interface to query those embeddings using OpenAI and LangChain

This design keeps RAG isolated from core ETL and modeling logic, allowing teams to opt-in or extend it independently.
